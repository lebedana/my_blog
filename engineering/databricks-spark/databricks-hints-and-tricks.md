# Databricks/Spark hints and tricks

* %fs magic command - wrapper around dbutils.fs 

### Schema

* inferSchema - .option\("inferSchema", True\) - otherwice all columns will be string
* provide schema 

`from pyspark.sql.types import DoubleType, IntegerType, StringType, StructField, StructType`

`csvSchema = StructType([ StructField("index", IntegerType()), StructField("sepal_length", DoubleType()), StructField("sepal_width", DoubleType()), StructField("petal_length", DoubleType()), StructField("petal_width", DoubleType()), StructField("species", StringType()) ])`

Pak:

`schema(csvSchema) # Use the specified schema`

### Tables and view

* spark.catalog.listTables\(\) - show tables and views 
* **managed tables** - create perm table & copies data to efs vs **unmanaged tables** - files are not copied to dbfs 

  * performance, cost, and security implications can be significant. In almost all use cases, **UNmanaged tables are preferred.**

  \*\*\*\*

**Examine table**

`DESCRIBE EXTENDED tablename`

### DBFS root

The default storage location in DBFS is known as the _DBFS root_. Several types of data are stored in the following DBFS root locations:

* `/FileStore`: Imported data files, generated plots, and uploaded libraries. See [FileStore](https://docs.databricks.com/data/filestore.html#filestore).
* `/databricks-datasets`: Sample [public datasets](https://docs.databricks.com/data/databricks-datasets.html#databricks-datasets).
* `/databricks-results`: Files generated by downloading the [full results](https://docs.databricks.com/notebooks/notebooks-use.html#download-full-results) of a query.
* `/databricks/init`: Global and cluster-named \(deprecated\) [init scripts](https://docs.databricks.com/clusters/init-scripts.html).
* `/user/hive/warehouse`: Data and metadata for non-external Hive tables.

### Secrets

**1**. Create Key Vault & secrete keys in it

**2. Link DBX worskapce with Key Vault** he first step is to open a new web browser tab and navigate to `https://<your_azure_databricks_url>#secrets/createScope`

* The number after the `?o=` is the unique workspace identifier; append `#secrets/createScope` to this.

3. Access the secret by the key - ðŸ”‘  print\(dbutils.secrets.get\(scope="students", key="storageread"\)\) - key is a particular secrete ID 

